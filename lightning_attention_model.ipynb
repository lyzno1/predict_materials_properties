{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from pymatgen.core.periodic_table import Element\n",
    "from pymatgen.core.structure import Structure\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# bash megnet_orig.sh\n",
    "parser = argparse.ArgumentParser(description='liu_attention')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='number of batch size')\n",
    "parser.add_argument('--gamma', type=float, default=1.8, help='Gamma value for RBF')\n",
    "parser.add_argument('--cutoff', type=int, default=3, help='Cutoff length for triplets')\n",
    "# parser.add_argument(\"--max_length\", type=int, default=96, help=\"Maximum length parameter\")\n",
    "# parser.add_argument('--e', type=int, default=0, help='number of node embedding dim')\n",
    "\n",
    "args = parser.parse_args()"
   ],
   "id": "4e40b6d719a2b8b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_atom_distance(structure, atom_i, atom_j):  # 获取元素之间的距离\n",
    "    \"\"\"\n",
    "        计算两个原子之间的距离\n",
    "        Args:\n",
    "            structure (Structure): pymatgen Structure 对象\n",
    "            atom_i (int): 第一个原子的索引\n",
    "            atom_j (int): 第二个原子的索引\n",
    "        Returns:\n",
    "            distance (float): 两个原子之间的距离\n",
    "        \"\"\"\n",
    "    site_i = structure[atom_i]\n",
    "    site_j = structure[atom_j]\n",
    "    distance = site_i.distance(site_j)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def get_atomic_number(symbol):  # 元素符号转化为原子序数\n",
    "    return Element(symbol).number"
   ],
   "id": "a601c21230f5088d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TripletStats: # 统计三元组数量\n",
    "    def __init__(self, structures):\n",
    "        self.structures = structures\n",
    "        self.triplet_counts = self.calculate_triplet_counts()\n",
    "        self.average = self.calculate_average()\n",
    "        self.max_value = max(self.triplet_counts)\n",
    "        self.min_value = min(self.triplet_counts)\n",
    "        self.median = self.calculate_median()\n",
    "        self.most_common = self.calculate_most_common()\n",
    "        self.least_common = self.calculate_least_common()\n",
    "        self.new_max, self.new_min = self.calculate_trimmed_extremes()\n",
    "\n",
    "    def calculate_triplet_counts(self):\n",
    "        triplet_counts = []\n",
    "        for structure in self.structures:\n",
    "            len_triplet = (len(structure) * (len(structure) - 1)) // 2\n",
    "            triplet_counts.append(len_triplet)\n",
    "        return triplet_counts\n",
    "\n",
    "    def calculate_average(self):\n",
    "        return sum(self.triplet_counts) / len(self.triplet_counts)\n",
    "\n",
    "    def calculate_median(self):\n",
    "        sorted_counts = sorted(self.triplet_counts)\n",
    "        n = len(sorted_counts)\n",
    "        if n % 2 == 1:\n",
    "            return sorted_counts[n // 2]\n",
    "        else:\n",
    "            return (sorted_counts[n // 2 - 1] + sorted_counts[n // 2]) / 2\n",
    "\n",
    "    def calculate_most_common(self):\n",
    "        from collections import Counter\n",
    "        count = Counter(self.triplet_counts)\n",
    "        return count.most_common(1)[0][0]\n",
    "\n",
    "    def calculate_least_common(self):\n",
    "        from collections import Counter\n",
    "        count = Counter(self.triplet_counts)\n",
    "        return count.most_common()[-1][0]\n",
    "\n",
    "    def calculate_trimmed_extremes(self):\n",
    "        trimmed_counts = [x for x in self.triplet_counts if x != self.max_value and x != self.min_value]\n",
    "        if trimmed_counts:\n",
    "            new_max = max(trimmed_counts)\n",
    "            new_min = min(trimmed_counts)\n",
    "            return new_max, new_min\n",
    "        else:\n",
    "            return None, None  # 当所有值都相同时，去除后为空列表\n",
    "\n",
    "    def get_max_value(self):\n",
    "        print(\"最大值:\", self.max_value)\n",
    "        return int(self.max_value)\n",
    "\n",
    "    def get_min_value(self):\n",
    "        print(\"最小值:\", self.min_value)\n",
    "        return int(self.min_value)\n",
    "\n",
    "    def get_median(self):\n",
    "        print(\"中位数:\", self.median)\n",
    "        return int(self.median)\n",
    "\n",
    "    def get_average(self):\n",
    "        print(\"平均数:\", self.average)\n",
    "        return int(self.average)\n",
    "\n",
    "    def get_most_common(self):\n",
    "        print(\"出现最多的数:\", self.most_common)\n",
    "        return int(self.most_common)\n",
    "\n",
    "    def get_least_common(self):\n",
    "        print(\"出现最少的数:\", self.least_common)\n",
    "        return int(self.least_common)\n",
    "\n",
    "    def get_new_max(self):\n",
    "        print(\"去除最大最小值之后的最大值:\", self.new_max)\n",
    "        return int(self.new_max)\n",
    "\n",
    "    def get_new_min(self):\n",
    "        print(\"去除最大最小值之后的最小值:\", self.new_min)\n",
    "        return int(self.new_min)"
   ],
   "id": "a98d268b037093d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_triplets(structures, max_len):  # 处理成三元组\n",
    "    all_tensor_data = []  # 存储所有结构的三元组数据\n",
    "    for structure in structures:\n",
    "        tensor_data = []\n",
    "        num_atoms = len(structure)\n",
    "\n",
    "        if num_atoms == 1:\n",
    "            lattice = structure.lattice\n",
    "            atom_symbol = structure[0].species_string\n",
    "            atomic_number = get_atomic_number(atom_symbol)\n",
    "            triplet_data = (atomic_number, atomic_number, lattice.a)\n",
    "            tensor_data.append(triplet_data)\n",
    "            triplet_data = (atomic_number, atomic_number, lattice.b)\n",
    "            tensor_data.append(triplet_data)\n",
    "            triplet_data = (atomic_number, atomic_number, lattice.c)\n",
    "            tensor_data.append(triplet_data)\n",
    "            all_tensor_data.append(tensor_data)\n",
    "            continue\n",
    "\n",
    "        for i in range(num_atoms):\n",
    "            element_i = structure[i].species_string\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                element_j = structure[j].species_string\n",
    "                distance = get_atom_distance(structure, i, j)\n",
    "\n",
    "                # 将原子转换为对应的原子序数\n",
    "                atomic_number_i = get_atomic_number(element_i)\n",
    "                atomic_number_j = get_atomic_number(element_j)\n",
    "                # 存储原始的三元组数据\n",
    "                triplet_data = (atomic_number_i, atomic_number_j, distance)\n",
    "                tensor_data.append(triplet_data)\n",
    "\n",
    "        # 对三元组列表按照最后一个元素（距离信息）进行升序排序\n",
    "        tensor_data.sort(key=lambda x: x[2], reverse=False)\n",
    "\n",
    "        # 截断数据到max_length长度\n",
    "        if len(tensor_data) > max_len:\n",
    "            tensor_data = tensor_data[:max_len]\n",
    "        # 将当前结构的三元组数据添加到总列表中\n",
    "        all_tensor_data.append(tensor_data)\n",
    "\n",
    "    # 对不足最大长度的子列表进行补充\n",
    "    for sublist in all_tensor_data:\n",
    "        while len(sublist) < max_len:\n",
    "            sublist.append((0, 0, 0.0))\n",
    "\n",
    "    return all_tensor_data"
   ],
   "id": "ba911801e6305e0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BondExpansionRBF(nn.Module):\n",
    "    def __init__(self, num_features: int = 10, gamma: float = 1.0):\n",
    "        super(BondExpansionRBF, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, bond_dist: torch.Tensor) -> torch.Tensor:\n",
    "        # 生成特征中心张量\n",
    "        feature_centers = torch.arange(1, self.num_features + 1, device=bond_dist.device).float()\n",
    "\n",
    "        # 计算每个距离到各个特征中心的欧几里得距离\n",
    "        distance_to_centers = torch.abs(feature_centers - bond_dist.unsqueeze(-1))\n",
    "\n",
    "        # 使用高斯径向基函数计算每个距离对应的特征值\n",
    "        rbf_values = torch.exp(-self.gamma * distance_to_centers ** 2).squeeze()\n",
    "\n",
    "        return rbf_values\n",
    "\n",
    "class BondExpansionLearnable(nn.Module):\n",
    "    def __init__(self, num_features: int = 10, gamma: float = 1.0):\n",
    "        super(BondExpansionLearnable, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.centers = nn.Parameter(torch.randn(num_features))\n",
    "        self.gamma = gamma #nn.Parameter(torch.one(1))\n",
    "\n",
    "    def __call__(self, bond_dist: torch.Tensor) -> torch.Tensor:\n",
    "        distance_to_centers = torch.abs(self.centers - bond_dist.unsqueeze(-1))\n",
    "        rbf_values = torch.exp(-self.gamma * distance_to_centers ** 2)\n",
    "\n",
    "        return rbf_values\n",
    "\n",
    "\n",
    "class BondExpansionDynamicLearnable(nn.Module):\n",
    "    def __init__(self, num_features: int = 10):\n",
    "        super(BondExpansionDynamicLearnable, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.center_network = nn.Linear(1, num_features)\n",
    "        self.gamma_network = nn.Linear(1, num_features)\n",
    "        self.fc = nn.Linear(num_features, num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __call__(self, bond_dist: torch.Tensor) -> torch.Tensor:\n",
    "        bond_dist = bond_dist.unsqueeze(-1)\n",
    "        centers = self.center_network(bond_dist)\n",
    "        gammas = torch.exp(self.gamma_network(bond_dist))\n",
    "\n",
    "        distance_to_centers = torch.abs(centers - bond_dist)\n",
    "        rbf_values = torch.exp(-gammas * distance_to_centers ** 2)\n",
    "\n",
    "        rbf_values = self.fc(rbf_values)\n",
    "        rbf_values = self.relu(rbf_values)\n",
    "\n",
    "        return rbf_values.squeeze(-1)\n",
    "\n",
    "\n",
    "class BondExpansionAdjustableGamma(nn.Module):\n",
    "    def __init__(self, num_features: int = 10):\n",
    "        super(BondExpansionAdjustableGamma, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def __call__(self, bond_dist: torch.Tensor) -> torch.Tensor:\n",
    "        feature_centers = torch.arange(1, self.num_features + 1, device=bond_dist.device).float()\n",
    "        distance_to_centers = torch.abs(feature_centers - bond_dist.unsqueeze(-1))\n",
    "        rbf_values = torch.exp(-self.gamma * distance_to_centers ** 2)\n",
    "\n",
    "        return rbf_values"
   ],
   "id": "58c838b033680436"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class StructureDataset(Dataset):\n",
    "    def __init__(self, structure, target):\n",
    "        self.input = structure\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        target = self.target.iloc[idx]\n",
    "\n",
    "        # 返回合金描述和目标值\n",
    "        return {'input': input, 'target': target}"
   ],
   "id": "21a1e051db5518ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Attention_structer_model(nn.Module):\n",
    "    def __init__(self, embedding_dim=10, hidden_size=64, output_size=1, dropout=0.2, num_features=10):\n",
    "        super(Attention_structer_model, self).__init__()\n",
    "        self.embedding = nn.Embedding(119, embedding_dim)\n",
    "        self.bond_expansion = BondExpansionRBF(num_features=num_features, gamma=args.gamma)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=embedding_dim * 3, hidden_size=hidden_size, batch_first=True, num_layers=3,\n",
    "                          dropout=dropout)\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=2, batch_first=True)\n",
    "\n",
    "        self.linear_feed = nn.Linear(hidden_size, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear_forward = nn.Linear(1024, hidden_size)\n",
    "\n",
    "        self.self_attention1 = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=2, batch_first=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(32, output_size)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm3 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm4 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 嵌入元素和键扩展\n",
    "        atom1 = self.embedding(x[:, :, 0].to(torch.long))\n",
    "        atom2 = self.embedding(x[:, :, 1].to(torch.long))\n",
    "        bond = self.bond_expansion(x[:, :, 2].float())\n",
    "        embedded_data = torch.cat([atom1, atom2, bond], dim=-1)\n",
    "\n",
    "        # shape: (batch_size, seq_len, input_size)\n",
    "        gru_output, _ = self.gru(embedded_data)\n",
    "\n",
    "        # 调用 self-attention\n",
    "        attention_output, _ = self.self_attention(gru_output, gru_output, gru_output)\n",
    "        attention_output = self.layer_norm1(gru_output + attention_output)\n",
    "\n",
    "        feed_forward_output = self.linear_feed(attention_output)\n",
    "        feed_forward_output = self.relu1(feed_forward_output)\n",
    "        feed_forward_output = self.linear_forward(feed_forward_output)\n",
    "        attention_output = self.layer_norm2(attention_output + feed_forward_output)\n",
    "\n",
    "        attention_output, _ = self.self_attention(attention_output, attention_output, attention_output)\n",
    "        attention_output = self.layer_norm3(attention_output + attention_output)\n",
    "\n",
    "        feed_forward_output = self.linear_feed(attention_output)\n",
    "        feed_forward_output = self.relu1(feed_forward_output)\n",
    "        feed_forward_output = self.linear_forward(feed_forward_output)\n",
    "        attention_output = self.layer_norm4(attention_output + feed_forward_output)\n",
    "\n",
    "        # 调用 self-attention1\n",
    "        attention_output, _ = self.self_attention1(attention_output, attention_output, attention_output)\n",
    "\n",
    "        output = attention_output[:, -1, :]  # 选择最后一个时间步的隐藏状态\n",
    "\n",
    "        # 全连接层\n",
    "        output = self.linear(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.linear1(output)\n",
    "\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        return output"
   ],
   "id": "15122d0c3872d75d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class liu_attention_Lightning(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim=10, hidden_size=64, output_size=1, dropout=0, num_features=10):\n",
    "        super(liu_attention_Lightning, self).__init__()\n",
    "        self.model = Attention_structer_model(embedding_dim=embedding_dim, hidden_size=hidden_size,\n",
    "                                              output_size=output_size, dropout=dropout, num_features=num_features)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, label = batch['input'], batch['target'].float()\n",
    "        predict = self.model(x)\n",
    "        loss = F.l1_loss(predict, label)\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, label = batch['input'], batch['target'].float()\n",
    "        predict = self.model(x)\n",
    "        val_loss = F.l1_loss(predict, label)\n",
    "        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, label = batch['input'], batch['target'].float()\n",
    "        predict = self.model(x)\n",
    "\n",
    "        test_loss = F.l1_loss(predict, label)\n",
    "        self.log('test_mae', test_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "id": "16a9b5d90f5ae28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_results(results_list, mb_dataset_name): # 可视化结果并保存到文件中\n",
    "    for i, mae in enumerate(results_list):\n",
    "        print(f\"Fold {i} MAE: {mae}\")\n",
    "    average_mae = sum(mae_list) / len(mae_list)\n",
    "    print(f\"Average MAE across all folds: {average_mae}\")\n",
    "\n",
    "    # 写入结果到文件\n",
    "    with open('results1.txt', 'a') as f:\n",
    "        if f.tell() != 0:\n",
    "            f.write('\\n')\n",
    "        for fold_num, mae in enumerate(results_list):\n",
    "            f.write(f\"Fold {fold_num}, MAE:{mae}\\n\")\n",
    "        f.write(f\"{mb_dataset_name}, batch_size:{batch_size}, gamma:{args.gamma}, Average MAE: {average_mae}\\n\")\n",
    "    results_list.clear()"
   ],
   "id": "16558dd61bb44c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def set_random_seed(seed): # 固定随机种子\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ],
   "id": "5ac9d6fa8d7e7fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(0)\n",
    "\n",
    "    init_seed = 42\n",
    "    set_random_seed(init_seed)"
   ],
   "id": "b72e20f3532f09b8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "    mb = MatbenchBenchmark(\n",
    "        autoload=False,\n",
    "        subset=[\n",
    "            \"matbench_phonons\",  # 1,265\n",
    "            \"matbench_jdft2d\",  # 636\n",
    "            \"matbench_dielectric\",  # 4,764\n",
    "            \"matbench_perovskites\",  # 1w8\n",
    "            \"matbench_log_gvrh\",  # 10,987\n",
    "            \"matbench_log_kvrh\",  # 10,987\n",
    "            # \"matbench_mp_gap\",   # 回归 10.6w\n",
    "            # \"matbench_mp_e_form\",  # 回归 13w\n",
    "        ]\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(0)\n",
    "    # 保存每个fold的MAE\n",
    "    mae_list = []\n",
    "\n",
    "    for task in mb.tasks:\n",
    "        set_random_seed(init_seed)\n",
    "        task.load()\n",
    "        dataset_name = task.dataset_name\n",
    "        for fold in task.folds:\n",
    "\n",
    "            train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "            max_length = TripletStats(train_inputs).get_average() * args.cutoff # 用于截断/补齐\n",
    "            # max_length = args.max_length\n",
    "            x_input = torch.tensor(get_triplets(train_inputs, max_length))  # 处理输入\n",
    "\n",
    "            dataset = StructureDataset(x_input, train_outputs)\n",
    "            train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "            batch_size = args.batch_size\n",
    "            num_worker = 4\n",
    "            train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_worker,\n",
    "                                      persistent_workers=True)\n",
    "            val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_worker,\n",
    "                                    persistent_workers=True)\n",
    "\n",
    "            lightning_model = liu_attention_Lightning()\n",
    "\n",
    "            early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=300, verbose=True,\n",
    "                                                mode=\"min\")\n",
    "            checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "\n",
    "            trainer = pl.Trainer(max_epochs=2000, callbacks=[early_stop_callback,checkpoint_callback],\n",
    "                                 log_every_n_steps=50)\n",
    "            trainer.fit(model=lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "            # 加载验证损失最小的模型权重\n",
    "            best_model_path = checkpoint_callback.best_model_path\n",
    "            lightning_model = liu_attention_Lightning.load_from_checkpoint(best_model_path)\n",
    "\n",
    "            # 保存最佳模型到 .pth 文件，文件名包含fold编号\n",
    "            torch.save(lightning_model.state_dict(), f'phonons_model_fold_{fold}.pth')\n",
    "\n",
    "            # 测试\n",
    "            lightning_model.eval()\n",
    "            test_inputs, test_outputs = task.get_test_data(fold, include_target=True)\n",
    "            test_inputs = torch.tensor(get_triplets(test_inputs, max_length))\n",
    "            test_dataset = StructureDataset(test_inputs, test_outputs)\n",
    "            test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_worker,\n",
    "                                     persistent_workers=True)\n",
    "\n",
    "            predict = trainer.test(model=lightning_model, dataloaders=test_loader)\n",
    "\n",
    "            mae = predict[0]['test_mae']\n",
    "            mae_list.append(mae)\n",
    "\n",
    "        visualize_results(mae_list, dataset_name)"
   ],
   "id": "b5dcea2c88924e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "91260d76bcb28970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a6fa690b054cc01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5087b7e397fb577"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
